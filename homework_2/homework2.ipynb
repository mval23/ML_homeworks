{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216e47b8",
   "metadata": {},
   "source": [
    "# Autoencoder para MNIST con Gradient Accumulation\n",
    "\n",
    "## Descripción del Proyecto\n",
    "\n",
    "Este notebook implementa un Autoencoder neuronal para el dataset MNIST utilizando PyTorch. La implementación incluye:\n",
    "\n",
    "1. **Arquitectura de Autoencoder**: Red neuronal con encoder y decoder para reconstrucción de imágenes\n",
    "2. **Gradient Accumulation**: Técnica para simular batch sizes más grandes sin incrementar el uso de memoria\n",
    "3. **Training Loop Completo**: Implementación profesional con métricas, logging y validación\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "- Construir una arquitectura de Autoencoder eficiente para MNIST\n",
    "- Implementar Gradient Accumulation para optimizar el entrenamiento\n",
    "- Evaluar la calidad de reconstrucción de las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7f816f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizando dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "# Importación de librerías necesarias\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import Tuple, Dict, List\n",
    "import time\n",
    "\n",
    "# Configuración de dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilizando dispositivo: {device}\")\n",
    "\n",
    "# Configuración para reproducibilidad\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8a2e5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración del modelo:\n",
      "- Batch size: 128\n",
      "- Batch size efectivo (con accumulation): 512\n",
      "- Accumulation steps: 4\n",
      "- Dimensión latente: 32\n",
      "- Learning rate: 0.001\n",
      "- Épocas: 50\n"
     ]
    }
   ],
   "source": [
    "# Configuración e Hiperparámetros\n",
    "class Config:\n",
    "    \"\"\"Clase para centralizar la configuración del modelo\"\"\"\n",
    "    \n",
    "    # Hiperparámetros del dataset\n",
    "    BATCH_SIZE = 128\n",
    "    NUM_WORKERS = 4\n",
    "    \n",
    "    # Hiperparámetros del modelo\n",
    "    INPUT_DIM = 28 * 28  # Dimensión de entrada (784 para MNIST)\n",
    "    HIDDEN_DIMS = [512, 256, 128, 64]  # Dimensiones de las capas ocultas\n",
    "    LATENT_DIM = 32  # Dimensión del espacio latente\n",
    "    \n",
    "    # Hiperparámetros de entrenamiento\n",
    "    LEARNING_RATE = 1e-3\n",
    "    NUM_EPOCHS = 50\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    \n",
    "    # Gradient Accumulation\n",
    "    ACCUMULATION_STEPS = 4  # Acumular gradientes por 4 steps\n",
    "    EFFECTIVE_BATCH_SIZE = BATCH_SIZE * ACCUMULATION_STEPS  # Batch size efectivo\n",
    "    \n",
    "    # Logging y guardado\n",
    "    LOG_INTERVAL = 100  # Cada cuántos batches loggear\n",
    "    SAVE_INTERVAL = 10  # Cada cuántas épocas guardar el modelo\n",
    "    \n",
    "    # Directorio para guardar resultados\n",
    "    SAVE_DIR = \"autoencoder_results\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Crear directorio para resultados\n",
    "os.makedirs(config.SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Configuración del modelo:\")\n",
    "print(f\"- Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"- Batch size efectivo (con accumulation): {config.EFFECTIVE_BATCH_SIZE}\")\n",
    "print(f\"- Accumulation steps: {config.ACCUMULATION_STEPS}\")\n",
    "print(f\"- Dimensión latente: {config.LATENT_DIM}\")\n",
    "print(f\"- Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"- Épocas: {config.NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24d631de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset MNIST...\n",
      "Dataset cargado exitosamente:\n",
      "- Batches de entrenamiento: 469\n",
      "- Batches de validación: 79\n",
      "- Tamaño total entrenamiento: 60000\n",
      "- Tamaño total validación: 10000\n",
      "Dataset cargado exitosamente:\n",
      "- Batches de entrenamiento: 469\n",
      "- Batches de validación: 79\n",
      "- Tamaño total entrenamiento: 60000\n",
      "- Tamaño total validación: 10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Carga y Preprocesamiento del Dataset MNIST\n",
    "\n",
    "def get_mnist_dataloaders(batch_size: int = 128, num_workers: int = 4) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Carga el dataset MNIST y retorna los dataloaders para entrenamiento y validación.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Tamaño del batch\n",
    "        num_workers: Número de workers para el DataLoader\n",
    "        \n",
    "    Returns:\n",
    "        Tuple con dataloaders de entrenamiento y validación\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transformaciones para normalizar las imágenes\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # Media y std de MNIST\n",
    "    ])\n",
    "    \n",
    "    # Cargar datasets\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data', \n",
    "        train=True, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data', \n",
    "        train=False, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Crear dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Cargar los datos\n",
    "print(\"Cargando dataset MNIST...\")\n",
    "train_loader, test_loader = get_mnist_dataloaders(\n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    num_workers=config.NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(f\"Dataset cargado exitosamente:\")\n",
    "print(f\"- Batches de entrenamiento: {len(train_loader)}\")\n",
    "print(f\"- Batches de validación: {len(test_loader)}\")\n",
    "print(f\"- Tamaño total entrenamiento: {len(train_loader.dataset)}\")\n",
    "print(f\"- Tamaño total validación: {len(test_loader.dataset)}\")\n",
    "\n",
    "# Visualizar algunas muestras\n",
    "def visualize_samples(dataloader: DataLoader, num_samples: int = 8):\n",
    "    \"\"\"Visualiza muestras del dataset\"\"\"\n",
    "    \n",
    "    data_iter = iter(dataloader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(12, 2))\n",
    "    for i in range(num_samples):\n",
    "        img = images[i].squeeze().numpy()\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f'Label: {labels[i].item()}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Muestras del Dataset MNIST')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d19e5120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquitectura del Autoencoder:\n",
      "Autoencoder(\n",
      "  (encoder): Encoder(\n",
      "    (encoder): Sequential(\n",
      "      (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Dropout(p=0.2, inplace=False)\n",
      "      (8): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): ReLU(inplace=True)\n",
      "      (11): Dropout(p=0.2, inplace=False)\n",
      "      (12): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (13): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (14): ReLU(inplace=True)\n",
      "      (15): Dropout(p=0.2, inplace=False)\n",
      "      (16): Linear(in_features=64, out_features=32, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "      (4): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Dropout(p=0.2, inplace=False)\n",
      "      (8): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): ReLU(inplace=True)\n",
      "      (11): Dropout(p=0.2, inplace=False)\n",
      "      (12): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (14): ReLU(inplace=True)\n",
      "      (15): Dropout(p=0.2, inplace=False)\n",
      "      (16): Linear(in_features=512, out_features=784, bias=True)\n",
      "      (17): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Número total de parámetros: 1,157,552\n",
      "\n",
      "Flujo de información:\n",
      "Entrada: 784 -> 512 -> 256 -> 128 -> 64 -> 32 (latente)\n",
      "Latente: 32 -> 64 -> 128 -> 256 -> 512 -> 784 (reconstrucción)\n"
     ]
    }
   ],
   "source": [
    "# Arquitectura del Autoencoder\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder del Autoencoder.\n",
    "    Comprime la entrada de alta dimensión a un espacio latente de menor dimensión.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int], latent_dim: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Dimensión de entrada (784 para MNIST)\n",
    "            hidden_dims: Lista con las dimensiones de las capas ocultas\n",
    "            latent_dim: Dimensión del espacio latente\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Construir las capas del encoder\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "            current_dim = hidden_dim\n",
    "        \n",
    "        # Capa final hacia el espacio latente\n",
    "        layers.append(nn.Linear(current_dim, latent_dim))\n",
    "        \n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass del encoder\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor de entrada de forma (batch_size, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Representación en el espacio latente\n",
    "        \"\"\"\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder del Autoencoder.\n",
    "    Reconstruye la entrada original desde el espacio latente.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim: int, hidden_dims: List[int], output_dim: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            latent_dim: Dimensión del espacio latente\n",
    "            hidden_dims: Lista con las dimensiones de las capas ocultas (en orden inverso)\n",
    "            output_dim: Dimensión de salida (784 para MNIST)\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Construir las capas del decoder (orden inverso)\n",
    "        layers = []\n",
    "        current_dim = latent_dim\n",
    "        \n",
    "        # Invertir el orden de las dimensiones ocultas\n",
    "        reversed_hidden_dims = hidden_dims[::-1]\n",
    "        \n",
    "        for hidden_dim in reversed_hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "            current_dim = hidden_dim\n",
    "        \n",
    "        # Capa final hacia la salida\n",
    "        layers.extend([\n",
    "            nn.Linear(current_dim, output_dim),\n",
    "            nn.Tanh()  # Tanh para que la salida esté en [-1, 1]\n",
    "        ])\n",
    "        \n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass del decoder\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor del espacio latente de forma (batch_size, latent_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Reconstrucción de la entrada original\n",
    "        \"\"\"\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder completo que combina encoder y decoder.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int], latent_dim: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Dimensión de entrada\n",
    "            hidden_dims: Dimensiones de las capas ocultas\n",
    "            latent_dim: Dimensión del espacio latente\n",
    "        \"\"\"\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = Encoder(input_dim, hidden_dims, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dims, input_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass completo del autoencoder\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor de entrada\n",
    "            \n",
    "        Returns:\n",
    "            Tuple con (reconstrucción, representación_latente)\n",
    "        \"\"\"\n",
    "        # Aplanar la entrada si es necesario\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        # Encoding\n",
    "        latent = self.encoder(x)\n",
    "        \n",
    "        # Decoding\n",
    "        reconstruction = self.decoder(latent)\n",
    "        \n",
    "        return reconstruction, latent\n",
    "    \n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Codificar entrada al espacio latente\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Decodificar desde el espacio latente\"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "\n",
    "# Crear el modelo\n",
    "model = Autoencoder(\n",
    "    input_dim=config.INPUT_DIM,\n",
    "    hidden_dims=config.HIDDEN_DIMS,\n",
    "    latent_dim=config.LATENT_DIM\n",
    ").to(device)\n",
    "\n",
    "# Mostrar información del modelo\n",
    "def count_parameters(model):\n",
    "    \"\"\"Cuenta el número de parámetros entrenables\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Arquitectura del Autoencoder:\")\n",
    "print(model)\n",
    "print(f\"\\nNúmero total de parámetros: {count_parameters(model):,}\")\n",
    "\n",
    "# Mostrar el flujo de información\n",
    "print(f\"\\nFlujo de información:\")\n",
    "print(f\"Entrada: {config.INPUT_DIM} -> \", end=\"\")\n",
    "for dim in config.HIDDEN_DIMS:\n",
    "    print(f\"{dim} -> \", end=\"\")\n",
    "print(f\"{config.LATENT_DIM} (latente)\")\n",
    "print(f\"Latente: {config.LATENT_DIM} -> \", end=\"\")\n",
    "for dim in reversed(config.HIDDEN_DIMS):\n",
    "    print(f\"{dim} -> \", end=\"\")\n",
    "print(f\"{config.INPUT_DIM} (reconstrucción)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73d14f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Componentes de entrenamiento inicializados:\n",
      "- Función de pérdida: AutoencoderLoss\n",
      "- Optimizador: Adam (lr=0.001)\n",
      "- Scheduler: ReduceLROnPlateau\n",
      "- Peso de reconstrucción: 1.0\n",
      "- Peso de regularización: 1e-05\n"
     ]
    }
   ],
   "source": [
    "# Función de Pérdida y Métricas\n",
    "\n",
    "class AutoencoderLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Función de pérdida personalizada para el autoencoder.\n",
    "    Combina MSE Loss y regularización.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reconstruction_weight: float = 1.0, regularization_weight: float = 1e-5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            reconstruction_weight: Peso para la pérdida de reconstrucción\n",
    "            regularization_weight: Peso para la regularización\n",
    "        \"\"\"\n",
    "        super(AutoencoderLoss, self).__init__()\n",
    "        self.reconstruction_weight = reconstruction_weight\n",
    "        self.regularization_weight = regularization_weight\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, reconstruction: torch.Tensor, target: torch.Tensor, \n",
    "                latent: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Calcula la pérdida total del autoencoder\n",
    "        \n",
    "        Args:\n",
    "            reconstruction: Reconstrucción del autoencoder\n",
    "            target: Objetivo (entrada original)\n",
    "            latent: Representación latente\n",
    "            \n",
    "        Returns:\n",
    "            Diccionario con las pérdidas\n",
    "        \"\"\"\n",
    "        # Pérdida de reconstrucción (MSE)\n",
    "        reconstruction_loss = self.mse_loss(reconstruction, target.view(target.size(0), -1))\n",
    "        \n",
    "        # Regularización L2 del espacio latente\n",
    "        latent_regularization = torch.mean(torch.sum(latent ** 2, dim=1))\n",
    "        \n",
    "        # Pérdida total\n",
    "        total_loss = (self.reconstruction_weight * reconstruction_loss + \n",
    "                     self.regularization_weight * latent_regularization)\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'reconstruction_loss': reconstruction_loss,\n",
    "            'latent_regularization': latent_regularization\n",
    "        }\n",
    "\n",
    "\n",
    "def calculate_metrics(reconstruction: torch.Tensor, target: torch.Tensor) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calcula métricas adicionales para evaluar la calidad de reconstrucción\n",
    "    \n",
    "    Args:\n",
    "        reconstruction: Reconstrucción del autoencoder\n",
    "        target: Objetivo (entrada original)\n",
    "        \n",
    "    Returns:\n",
    "        Diccionario con métricas\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Aplanar tensores\n",
    "        recon_flat = reconstruction.view(reconstruction.size(0), -1)\n",
    "        target_flat = target.view(target.size(0), -1)\n",
    "        \n",
    "        # MSE\n",
    "        mse = F.mse_loss(recon_flat, target_flat).item()\n",
    "        \n",
    "        # MAE (Mean Absolute Error)\n",
    "        mae = F.l1_loss(recon_flat, target_flat).item()\n",
    "        \n",
    "        # PSNR (Peak Signal-to-Noise Ratio)\n",
    "        psnr = 20 * torch.log10(torch.tensor(1.0) / torch.sqrt(torch.tensor(mse)))\n",
    "        \n",
    "        # Correlación promedio\n",
    "        correlation = 0.0\n",
    "        for i in range(recon_flat.size(0)):\n",
    "            corr = torch.corrcoef(torch.stack([recon_flat[i], target_flat[i]]))[0, 1]\n",
    "            if not torch.isnan(corr):\n",
    "                correlation += corr.item()\n",
    "        correlation /= recon_flat.size(0)\n",
    "        \n",
    "        return {\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'psnr': psnr.item(),\n",
    "            'correlation': correlation\n",
    "        }\n",
    "\n",
    "\n",
    "# Inicializar función de pérdida y optimizador\n",
    "criterion = AutoencoderLoss(\n",
    "    reconstruction_weight=1.0,\n",
    "    regularization_weight=config.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=config.LEARNING_RATE,\n",
    "    weight_decay=config.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Scheduler para learning rate\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=5\n",
    ")\n",
    "\n",
    "print(\"Componentes de entrenamiento inicializados:\")\n",
    "print(f\"- Función de pérdida: AutoencoderLoss\")\n",
    "print(f\"- Optimizador: Adam (lr={config.LEARNING_RATE})\")\n",
    "print(f\"- Scheduler: ReduceLROnPlateau\")\n",
    "print(f\"- Peso de reconstrucción: {criterion.reconstruction_weight}\")\n",
    "print(f\"- Peso de regularización: {criterion.regularization_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5f8231e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones de entrenamiento implementadas:\n",
      "- train_epoch_with_accumulation: Entrenamiento con gradient accumulation\n",
      "- validate_epoch: Validación del modelo\n",
      "- TrainingLogger: Logging y visualización de métricas\n",
      "- Gradient accumulation configurado para 4 steps\n",
      "- Batch size efectivo: 512\n"
     ]
    }
   ],
   "source": [
    "# Training Loop con Gradient Accumulation\n",
    "\n",
    "class TrainingLogger:\n",
    "    \"\"\"Clase para manejar el logging durante el entrenamiento\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_metrics = []\n",
    "        self.val_metrics = []\n",
    "        self.learning_rates = []\n",
    "        \n",
    "    def log_epoch(self, epoch: int, train_loss: float, val_loss: float, \n",
    "                  train_metrics: Dict, val_metrics: Dict, lr: float):\n",
    "        \"\"\"Registra métricas de una época\"\"\"\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.train_metrics.append(train_metrics)\n",
    "        self.val_metrics.append(val_metrics)\n",
    "        self.learning_rates.append(lr)\n",
    "        \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Grafica el historial de entrenamiento\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        \n",
    "        epochs = range(1, len(self.train_losses) + 1)\n",
    "        \n",
    "        # Loss\n",
    "        axes[0, 0].plot(epochs, self.train_losses, 'b-', label='Train')\n",
    "        axes[0, 0].plot(epochs, self.val_losses, 'r-', label='Validation')\n",
    "        axes[0, 0].set_title('Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # MSE\n",
    "        train_mse = [m['mse'] for m in self.train_metrics]\n",
    "        val_mse = [m['mse'] for m in self.val_metrics]\n",
    "        axes[0, 1].plot(epochs, train_mse, 'b-', label='Train')\n",
    "        axes[0, 1].plot(epochs, val_mse, 'r-', label='Validation')\n",
    "        axes[0, 1].set_title('MSE')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('MSE')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # PSNR\n",
    "        train_psnr = [m['psnr'] for m in self.train_metrics]\n",
    "        val_psnr = [m['psnr'] for m in self.val_metrics]\n",
    "        axes[0, 2].plot(epochs, train_psnr, 'b-', label='Train')\n",
    "        axes[0, 2].plot(epochs, val_psnr, 'r-', label='Validation')\n",
    "        axes[0, 2].set_title('PSNR')\n",
    "        axes[0, 2].set_xlabel('Epoch')\n",
    "        axes[0, 2].set_ylabel('PSNR (dB)')\n",
    "        axes[0, 2].legend()\n",
    "        axes[0, 2].grid(True)\n",
    "        \n",
    "        # MAE\n",
    "        train_mae = [m['mae'] for m in self.train_metrics]\n",
    "        val_mae = [m['mae'] for m in self.val_metrics]\n",
    "        axes[1, 0].plot(epochs, train_mae, 'b-', label='Train')\n",
    "        axes[1, 0].plot(epochs, val_mae, 'r-', label='Validation')\n",
    "        axes[1, 0].set_title('MAE')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('MAE')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Correlation\n",
    "        train_corr = [m['correlation'] for m in self.train_metrics]\n",
    "        val_corr = [m['correlation'] for m in self.val_metrics]\n",
    "        axes[1, 1].plot(epochs, train_corr, 'b-', label='Train')\n",
    "        axes[1, 1].plot(epochs, val_corr, 'r-', label='Validation')\n",
    "        axes[1, 1].set_title('Correlation')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Correlation')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        # Learning Rate\n",
    "        axes[1, 2].plot(epochs, self.learning_rates, 'g-')\n",
    "        axes[1, 2].set_title('Learning Rate')\n",
    "        axes[1, 2].set_xlabel('Epoch')\n",
    "        axes[1, 2].set_ylabel('Learning Rate')\n",
    "        axes[1, 2].set_yscale('log')\n",
    "        axes[1, 2].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(config.SAVE_DIR, 'training_history.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def train_epoch_with_accumulation(model: nn.Module, train_loader: DataLoader, \n",
    "                                criterion: nn.Module, optimizer: optim.Optimizer,\n",
    "                                accumulation_steps: int, device: torch.device) -> Tuple[float, Dict]:\n",
    "    \"\"\"\n",
    "    Entrena una época con gradient accumulation\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo a entrenar\n",
    "        train_loader: DataLoader de entrenamiento\n",
    "        criterion: Función de pérdida\n",
    "        optimizer: Optimizador\n",
    "        accumulation_steps: Número de steps para acumular gradientes\n",
    "        device: Dispositivo de cómputo\n",
    "        \n",
    "    Returns:\n",
    "        Tuple con (pérdida_promedio, métricas_promedio)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_metrics = {'mse': 0.0, 'mae': 0.0, 'psnr': 0.0, 'correlation': 0.0}\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Inicializar gradientes\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(progress_bar):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        reconstruction, latent = model(data)\n",
    "        \n",
    "        # Calcular pérdida\n",
    "        loss_dict = criterion(reconstruction, data, latent)\n",
    "        loss = loss_dict['total_loss']\n",
    "        \n",
    "        # Escalar la pérdida por accumulation steps\n",
    "        loss = loss / accumulation_steps\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Actualizar cada accumulation_steps o en el último batch\n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            # Gradient clipping para estabilidad\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Actualizar parámetros\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Acumular métricas\n",
    "        total_loss += loss.item() * accumulation_steps  # Reescalar para métricas\n",
    "        \n",
    "        # Calcular métricas adicionales\n",
    "        metrics = calculate_metrics(reconstruction, data)\n",
    "        for key, value in metrics.items():\n",
    "            total_metrics[key] += value\n",
    "        \n",
    "        num_batches += 1\n",
    "        \n",
    "        # Actualizar progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{loss.item() * accumulation_steps:.4f}',\n",
    "            'MSE': f'{metrics[\"mse\"]:.4f}',\n",
    "            'PSNR': f'{metrics[\"psnr\"]:.2f}dB'\n",
    "        })\n",
    "    \n",
    "    # Promediar métricas\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_metrics = {key: value / num_batches for key, value in total_metrics.items()}\n",
    "    \n",
    "    return avg_loss, avg_metrics\n",
    "\n",
    "\n",
    "def validate_epoch(model: nn.Module, val_loader: DataLoader, \n",
    "                  criterion: nn.Module, device: torch.device) -> Tuple[float, Dict]:\n",
    "    \"\"\"\n",
    "    Valida una época\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo a validar\n",
    "        val_loader: DataLoader de validación\n",
    "        criterion: Función de pérdida\n",
    "        device: Dispositivo de cómputo\n",
    "        \n",
    "    Returns:\n",
    "        Tuple con (pérdida_promedio, métricas_promedio)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_metrics = {'mse': 0.0, 'mae': 0.0, 'psnr': 0.0, 'correlation': 0.0}\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc=\"Validation\")\n",
    "        \n",
    "        for data, _ in progress_bar:\n",
    "            data = data.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstruction, latent = model(data)\n",
    "            \n",
    "            # Calcular pérdida\n",
    "            loss_dict = criterion(reconstruction, data, latent)\n",
    "            loss = loss_dict['total_loss']\n",
    "            \n",
    "            # Acumular métricas\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calcular métricas adicionales\n",
    "            metrics = calculate_metrics(reconstruction, data)\n",
    "            for key, value in metrics.items():\n",
    "                total_metrics[key] += value\n",
    "            \n",
    "            num_batches += 1\n",
    "            \n",
    "            # Actualizar progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'MSE': f'{metrics[\"mse\"]:.4f}',\n",
    "                'PSNR': f'{metrics[\"psnr\"]:.2f}dB'\n",
    "            })\n",
    "    \n",
    "    # Promediar métricas\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_metrics = {key: value / num_batches for key, value in total_metrics.items()}\n",
    "    \n",
    "    return avg_loss, avg_metrics\n",
    "\n",
    "\n",
    "print(\"Funciones de entrenamiento implementadas:\")\n",
    "print(\"- train_epoch_with_accumulation: Entrenamiento con gradient accumulation\")\n",
    "print(\"- validate_epoch: Validación del modelo\")\n",
    "print(\"- TrainingLogger: Logging y visualización de métricas\")\n",
    "print(f\"- Gradient accumulation configurado para {config.ACCUMULATION_STEPS} steps\")\n",
    "print(f\"- Batch size efectivo: {config.EFFECTIVE_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "873d9965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función principal de entrenamiento implementada:\n",
      "- Gradient accumulation integrado\n",
      "- Logging completo de métricas\n",
      "- Guardado automático del mejor modelo\n",
      "- Checkpoints periódicos\n",
      "- Early stopping por learning rate\n",
      "- Visualización automática del progreso\n"
     ]
    }
   ],
   "source": [
    "# Bucle Principal de Entrenamiento\n",
    "\n",
    "def save_checkpoint(model: nn.Module, optimizer: optim.Optimizer, \n",
    "                   scheduler, epoch: int, loss: float, filepath: str):\n",
    "    \"\"\"Guarda un checkpoint del modelo\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': loss,\n",
    "        'config': config\n",
    "    }, filepath)\n",
    "\n",
    "\n",
    "def load_checkpoint(filepath: str, model: nn.Module, optimizer: optim.Optimizer, scheduler):\n",
    "    \"\"\"Carga un checkpoint del modelo\"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    return checkpoint['epoch'], checkpoint['loss']\n",
    "\n",
    "\n",
    "def train_autoencoder(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
    "                     criterion: nn.Module, optimizer: optim.Optimizer, scheduler,\n",
    "                     num_epochs: int, accumulation_steps: int, device: torch.device):\n",
    "    \"\"\"\n",
    "    Función principal de entrenamiento del autoencoder\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo a entrenar\n",
    "        train_loader: DataLoader de entrenamiento\n",
    "        val_loader: DataLoader de validación\n",
    "        criterion: Función de pérdida\n",
    "        optimizer: Optimizador\n",
    "        scheduler: Scheduler de learning rate\n",
    "        num_epochs: Número de épocas\n",
    "        accumulation_steps: Steps para gradient accumulation\n",
    "        device: Dispositivo de cómputo\n",
    "    \"\"\"\n",
    "    \n",
    "    logger = TrainingLogger()\n",
    "    best_val_loss = float('inf')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"INICIANDO ENTRENAMIENTO DEL AUTOENCODER\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Configuración:\")\n",
    "    print(f\"- Épocas: {num_epochs}\")\n",
    "    print(f\"- Batch size: {config.BATCH_SIZE}\")\n",
    "    print(f\"- Batch size efectivo: {config.EFFECTIVE_BATCH_SIZE}\")\n",
    "    print(f\"- Accumulation steps: {accumulation_steps}\")\n",
    "    print(f\"- Learning rate inicial: {config.LEARNING_RATE}\")\n",
    "    print(f\"- Dispositivo: {device}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        print(f\"\\nÉpoca {epoch}/{num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Entrenamiento\n",
    "        train_loss, train_metrics = train_epoch_with_accumulation(\n",
    "            model, train_loader, criterion, optimizer, accumulation_steps, device\n",
    "        )\n",
    "        \n",
    "        # Validación\n",
    "        val_loss, val_metrics = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Actualizar scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Logging\n",
    "        logger.log_epoch(epoch, train_loss, val_loss, train_metrics, val_metrics, current_lr)\n",
    "        \n",
    "        # Imprimir métricas de la época\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"\\\\nResultados de la época {epoch}:\")\n",
    "        print(f\"Tiempo: {epoch_time:.2f}s\")\n",
    "        print(f\"Learning Rate: {current_lr:.2e}\")\n",
    "        print(f\"Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "        print(f\"Train MSE: {train_metrics['mse']:.6f} | Val MSE: {val_metrics['mse']:.6f}\")\n",
    "        print(f\"Train PSNR: {train_metrics['psnr']:.2f}dB | Val PSNR: {val_metrics['psnr']:.2f}dB\")\n",
    "        print(f\"Train MAE: {train_metrics['mae']:.6f} | Val MAE: {val_metrics['mae']:.6f}\")\n",
    "        print(f\"Train Corr: {train_metrics['correlation']:.4f} | Val Corr: {val_metrics['correlation']:.4f}\")\n",
    "        \n",
    "        # Guardar mejor modelo\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = os.path.join(config.SAVE_DIR, 'best_model.pth')\n",
    "            save_checkpoint(model, optimizer, scheduler, epoch, val_loss, best_model_path)\n",
    "            print(f\"Nuevo mejor modelo guardado (Val Loss: {val_loss:.6f})\")\n",
    "        \n",
    "        # Guardar checkpoint periódicamente\n",
    "        if epoch % config.SAVE_INTERVAL == 0:\n",
    "            checkpoint_path = os.path.join(config.SAVE_DIR, f'checkpoint_epoch_{epoch}.pth')\n",
    "            save_checkpoint(model, optimizer, scheduler, epoch, val_loss, checkpoint_path)\n",
    "            print(f\"Checkpoint guardado en época {epoch}\")\n",
    "        \n",
    "        # Early stopping simple\n",
    "        if current_lr < 1e-7:\n",
    "            print(f\"\\\\nEarly stopping: Learning rate muy bajo ({current_lr:.2e})\")\n",
    "            break\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(\"ENTRENAMIENTO COMPLETADO\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Tiempo total: {total_time:.2f}s ({total_time/60:.1f} minutos)\")\n",
    "    print(f\"Mejor pérdida de validación: {best_val_loss:.6f}\")\n",
    "    print(f\"Modelo guardado en: {config.SAVE_DIR}\")\n",
    "    \n",
    "    # Graficar historial de entrenamiento\n",
    "    logger.plot_training_history()\n",
    "    \n",
    "    return logger\n",
    "\n",
    "\n",
    "print(\"Función principal de entrenamiento implementada:\")\n",
    "print(\"- Gradient accumulation integrado\")\n",
    "print(\"- Logging completo de métricas\")\n",
    "print(\"- Guardado automático del mejor modelo\")\n",
    "print(\"- Checkpoints periódicos\")\n",
    "print(\"- Early stopping por learning rate\")\n",
    "print(\"- Visualización automática del progreso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b47858d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando configuración antes del entrenamiento...\n",
      "Modelo: Autoencoder con 1,157,552 parámetros\n",
      "Dispositivo: cpu\n",
      "Datos: 469 batches de entrenamiento, 79 batches de validación\n",
      "Optimizador: Adam\n",
      "Función de pérdida: AutoencoderLoss\n",
      "Gradient accumulation: 4 steps\n",
      "\\nVerificando forward pass...\n",
      "Input shape: torch.Size([128, 1, 28, 28])\n",
      "Latent shape: torch.Size([128, 32])\n",
      "Reconstruction shape: torch.Size([128, 784])\n",
      "Input range: [-0.424, 2.821]\n",
      "Reconstruction range: [-0.093, 0.089]\n",
      "================================================================================\n",
      "INICIANDO ENTRENAMIENTO DEL AUTOENCODER\n",
      "================================================================================\n",
      "Configuración:\n",
      "- Épocas: 50\n",
      "- Batch size: 128\n",
      "- Batch size efectivo: 512\n",
      "- Accumulation steps: 4\n",
      "- Learning rate inicial: 0.001\n",
      "- Dispositivo: cpu\n",
      "================================================================================\n",
      "\n",
      "Época 1/50\n",
      "--------------------------------------------------\n",
      "Input shape: torch.Size([128, 1, 28, 28])\n",
      "Latent shape: torch.Size([128, 32])\n",
      "Reconstruction shape: torch.Size([128, 784])\n",
      "Input range: [-0.424, 2.821]\n",
      "Reconstruction range: [-0.093, 0.089]\n",
      "================================================================================\n",
      "INICIANDO ENTRENAMIENTO DEL AUTOENCODER\n",
      "================================================================================\n",
      "Configuración:\n",
      "- Épocas: 50\n",
      "- Batch size: 128\n",
      "- Batch size efectivo: 512\n",
      "- Accumulation steps: 4\n",
      "- Learning rate inicial: 0.001\n",
      "- Dispositivo: cpu\n",
      "================================================================================\n",
      "\n",
      "Época 1/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:34<00:00, 13.41it/s, Loss=0.5679, MSE=0.5677, PSNR=2.46dB] \n",
      "Training: 100%|██████████| 469/469 [00:34<00:00, 13.41it/s, Loss=0.5679, MSE=0.5677, PSNR=2.46dB]\n",
      "Validation: 100%|██████████| 79/79 [00:12<00:00,  6.46it/s, Loss=0.6242, MSE=0.6240, PSNR=2.05dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 1:\n",
      "Tiempo: 47.22s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.638619 | Val Loss: 0.546021\n",
      "Train MSE: 0.638497 | Val MSE: 0.545902\n",
      "Train PSNR: 1.98dB | Val PSNR: 2.65dB\n",
      "Train MAE: 0.487572 | Val MAE: 0.399669\n",
      "Train Corr: 0.6001 | Val Corr: 0.7032\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.546021)\n",
      "\n",
      "Época 2/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:36<00:00, 12.96it/s, Loss=0.5638, MSE=0.5636, PSNR=2.49dB]\n",
      "Training: 100%|██████████| 469/469 [00:36<00:00, 12.96it/s, Loss=0.5638, MSE=0.5636, PSNR=2.49dB]\n",
      "Validation: 100%|██████████| 79/79 [00:15<00:00,  5.06it/s, Loss=0.6035, MSE=0.6034, PSNR=2.19dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 2:\n",
      "Tiempo: 51.83s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.547643 | Val Loss: 0.508227\n",
      "Train MSE: 0.547504 | Val MSE: 0.508106\n",
      "Train PSNR: 2.62dB | Val PSNR: 2.96dB\n",
      "Train MAE: 0.420643 | Val MAE: 0.372555\n",
      "Train Corr: 0.6912 | Val Corr: 0.7447\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.508227)\n",
      "\n",
      "Época 3/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:54<00:00,  8.58it/s, Loss=0.5360, MSE=0.5358, PSNR=2.71dB]\n",
      "Validation:   0%|          | 0/79 [00:00<?, ?it/s]\n",
      "Validation: 100%|██████████| 79/79 [00:15<00:00,  4.98it/s, Loss=0.5754, MSE=0.5752, PSNR=2.40dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 3:\n",
      "Tiempo: 70.54s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.523258 | Val Loss: 0.490612\n",
      "Train MSE: 0.523121 | Val MSE: 0.490499\n",
      "Train PSNR: 2.82dB | Val PSNR: 3.12dB\n",
      "Train MAE: 0.395662 | Val MAE: 0.356744\n",
      "Train Corr: 0.7178 | Val Corr: 0.7654\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.490612)\n",
      "\n",
      "Época 4/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:36<00:00, 12.75it/s, Loss=0.5070, MSE=0.5069, PSNR=2.95dB]\n",
      "Training: 100%|██████████| 469/469 [00:36<00:00, 12.75it/s, Loss=0.5070, MSE=0.5069, PSNR=2.95dB]\n",
      "Validation: 100%|██████████| 79/79 [00:12<00:00,  6.47it/s, Loss=0.5608, MSE=0.5607, PSNR=2.51dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 4:\n",
      "Tiempo: 49.01s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.511447 | Val Loss: 0.479613\n",
      "Train MSE: 0.511311 | Val MSE: 0.479495\n",
      "Train PSNR: 2.92dB | Val PSNR: 3.21dB\n",
      "Train MAE: 0.382056 | Val MAE: 0.349236\n",
      "Train Corr: 0.7310 | Val Corr: 0.7749\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.479613)\n",
      "\n",
      "Época 5/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:34<00:00, 13.58it/s, Loss=0.4981, MSE=0.4979, PSNR=3.03dB]\n",
      "Training: 100%|██████████| 469/469 [00:34<00:00, 13.58it/s, Loss=0.4981, MSE=0.4979, PSNR=3.03dB]\n",
      "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.52it/s, Loss=0.5589, MSE=0.5588, PSNR=2.53dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 5:\n",
      "Tiempo: 45.05s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.503987 | Val Loss: 0.473957\n",
      "Train MSE: 0.503853 | Val MSE: 0.473846\n",
      "Train PSNR: 2.98dB | Val PSNR: 3.27dB\n",
      "Train MAE: 0.374324 | Val MAE: 0.346393\n",
      "Train Corr: 0.7393 | Val Corr: 0.7802\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.473957)\n",
      "\n",
      "Época 6/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:33<00:00, 14.02it/s, Loss=0.5080, MSE=0.5079, PSNR=2.94dB]\n",
      "Training: 100%|██████████| 469/469 [00:33<00:00, 14.02it/s, Loss=0.5080, MSE=0.5079, PSNR=2.94dB]\n",
      "Validation: 100%|██████████| 79/79 [00:11<00:00,  6.72it/s, Loss=0.5542, MSE=0.5541, PSNR=2.56dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 6:\n",
      "Tiempo: 45.23s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.499027 | Val Loss: 0.467512\n",
      "Train MSE: 0.498895 | Val MSE: 0.467403\n",
      "Train PSNR: 3.02dB | Val PSNR: 3.33dB\n",
      "Train MAE: 0.369968 | Val MAE: 0.342231\n",
      "Train Corr: 0.7448 | Val Corr: 0.7873\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.467512)\n",
      "\n",
      "Época 7/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:31<00:00, 14.69it/s, Loss=0.4658, MSE=0.4657, PSNR=3.32dB]\n",
      "Training: 100%|██████████| 469/469 [00:31<00:00, 14.69it/s, Loss=0.4658, MSE=0.4657, PSNR=3.32dB]\n",
      "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.60it/s, Loss=0.5494, MSE=0.5493, PSNR=2.60dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 7:\n",
      "Tiempo: 42.34s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.494547 | Val Loss: 0.463205\n",
      "Train MSE: 0.494418 | Val MSE: 0.463102\n",
      "Train PSNR: 3.06dB | Val PSNR: 3.37dB\n",
      "Train MAE: 0.366150 | Val MAE: 0.336143\n",
      "Train Corr: 0.7501 | Val Corr: 0.7967\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.463205)\n",
      "\n",
      "Época 8/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:34<00:00, 13.78it/s, Loss=0.5172, MSE=0.5171, PSNR=2.86dB]\n",
      "Training: 100%|██████████| 469/469 [00:34<00:00, 13.78it/s, Loss=0.5172, MSE=0.5171, PSNR=2.86dB]\n",
      "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.80it/s, Loss=0.5445, MSE=0.5443, PSNR=2.64dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 8:\n",
      "Tiempo: 44.16s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.491276 | Val Loss: 0.460049\n",
      "Train MSE: 0.491149 | Val MSE: 0.459950\n",
      "Train PSNR: 3.09dB | Val PSNR: 3.40dB\n",
      "Train MAE: 0.363332 | Val MAE: 0.336292\n",
      "Train Corr: 0.7540 | Val Corr: 0.7969\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.460049)\n",
      "\n",
      "Época 9/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:31<00:00, 14.92it/s, Loss=0.4867, MSE=0.4866, PSNR=3.13dB]\n",
      "Training: 100%|██████████| 469/469 [00:31<00:00, 14.92it/s, Loss=0.4867, MSE=0.4866, PSNR=3.13dB]\n",
      "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.19it/s, Loss=0.5440, MSE=0.5439, PSNR=2.64dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 9:\n",
      "Tiempo: 42.44s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.489091 | Val Loss: 0.457880\n",
      "Train MSE: 0.488966 | Val MSE: 0.457783\n",
      "Train PSNR: 3.11dB | Val PSNR: 3.42dB\n",
      "Train MAE: 0.361225 | Val MAE: 0.331959\n",
      "Train Corr: 0.7567 | Val Corr: 0.8020\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.457880)\n",
      "\n",
      "Época 10/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:34<00:00, 13.62it/s, Loss=0.4960, MSE=0.4959, PSNR=3.05dB]\n",
      "Training: 100%|██████████| 469/469 [00:34<00:00, 13.62it/s, Loss=0.4960, MSE=0.4959, PSNR=3.05dB]\n",
      "Validation: 100%|██████████| 79/79 [00:23<00:00,  3.35it/s, Loss=0.5423, MSE=0.5422, PSNR=2.66dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 10:\n",
      "Tiempo: 58.02s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.487035 | Val Loss: 0.454851\n",
      "Train MSE: 0.486911 | Val MSE: 0.454749\n",
      "Train PSNR: 3.13dB | Val PSNR: 3.45dB\n",
      "Train MAE: 0.359348 | Val MAE: 0.331865\n",
      "Train Corr: 0.7592 | Val Corr: 0.8019\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.454851)\n",
      "Checkpoint guardado en época 10\n",
      "\n",
      "Época 11/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [02:30<00:00,  3.11it/s, Loss=0.4605, MSE=0.4603, PSNR=3.37dB]\n",
      "Validation:   0%|          | 0/79 [00:00<?, ?it/s]\n",
      "Validation: 100%|██████████| 79/79 [00:16<00:00,  4.88it/s, Loss=0.5415, MSE=0.5413, PSNR=2.67dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 11:\n",
      "Tiempo: 166.87s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.485080 | Val Loss: 0.453272\n",
      "Train MSE: 0.484959 | Val MSE: 0.453173\n",
      "Train PSNR: 3.14dB | Val PSNR: 3.46dB\n",
      "Train MAE: 0.357580 | Val MAE: 0.328785\n",
      "Train Corr: 0.7614 | Val Corr: 0.8070\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.453272)\n",
      "\n",
      "Época 12/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [01:27<00:00,  5.37it/s, Loss=0.5046, MSE=0.5045, PSNR=2.97dB]\n",
      "Training: 100%|██████████| 469/469 [01:27<00:00,  5.37it/s, Loss=0.5046, MSE=0.5045, PSNR=2.97dB]\n",
      "Validation: 100%|██████████| 79/79 [00:29<00:00,  2.70it/s, Loss=0.5403, MSE=0.5402, PSNR=2.67dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 12:\n",
      "Tiempo: 116.63s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.483443 | Val Loss: 0.452100\n",
      "Train MSE: 0.483324 | Val MSE: 0.452005\n",
      "Train PSNR: 3.16dB | Val PSNR: 3.47dB\n",
      "Train MAE: 0.355952 | Val MAE: 0.329875\n",
      "Train Corr: 0.7635 | Val Corr: 0.8055\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.452100)\n",
      "\n",
      "Época 13/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:45<00:00, 10.39it/s, Loss=0.4939, MSE=0.4938, PSNR=3.06dB]\n",
      "Training: 100%|██████████| 469/469 [00:45<00:00, 10.39it/s, Loss=0.4939, MSE=0.4938, PSNR=3.06dB]\n",
      "Validation: 100%|██████████| 79/79 [00:17<00:00,  4.47it/s, Loss=0.5361, MSE=0.5360, PSNR=2.71dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 13:\n",
      "Tiempo: 62.86s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.481834 | Val Loss: 0.450506\n",
      "Train MSE: 0.481718 | Val MSE: 0.450411\n",
      "Train PSNR: 3.17dB | Val PSNR: 3.49dB\n",
      "Train MAE: 0.354351 | Val MAE: 0.329321\n",
      "Train Corr: 0.7654 | Val Corr: 0.8059\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.450506)\n",
      "\n",
      "Época 14/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:40<00:00, 11.58it/s, Loss=0.4978, MSE=0.4977, PSNR=3.03dB]\n",
      "Validation:   0%|          | 0/79 [00:00<?, ?it/s]\n",
      "Validation: 100%|██████████| 79/79 [00:14<00:00,  5.47it/s, Loss=0.5370, MSE=0.5369, PSNR=2.70dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 14:\n",
      "Tiempo: 54.97s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.480555 | Val Loss: 0.449385\n",
      "Train MSE: 0.480440 | Val MSE: 0.449290\n",
      "Train PSNR: 3.19dB | Val PSNR: 3.50dB\n",
      "Train MAE: 0.353003 | Val MAE: 0.325554\n",
      "Train Corr: 0.7669 | Val Corr: 0.8117\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.449385)\n",
      "\n",
      "Época 15/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:40<00:00, 11.59it/s, Loss=0.4996, MSE=0.4995, PSNR=3.01dB]\n",
      "Training: 100%|██████████| 469/469 [00:40<00:00, 11.59it/s, Loss=0.4996, MSE=0.4995, PSNR=3.01dB]\n",
      "Validation: 100%|██████████| 79/79 [00:15<00:00,  5.25it/s, Loss=0.5360, MSE=0.5359, PSNR=2.71dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 15:\n",
      "Tiempo: 55.53s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.479483 | Val Loss: 0.447995\n",
      "Train MSE: 0.479370 | Val MSE: 0.447903\n",
      "Train PSNR: 3.20dB | Val PSNR: 3.51dB\n",
      "Train MAE: 0.351973 | Val MAE: 0.325230\n",
      "Train Corr: 0.7682 | Val Corr: 0.8107\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.447995)\n",
      "\n",
      "Época 16/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [02:32<00:00,  3.07it/s, Loss=0.4777, MSE=0.4776, PSNR=3.21dB]\n",
      "Validation:   0%|          | 0/79 [00:00<?, ?it/s]\n",
      "Validation: 100%|██████████| 79/79 [01:13<00:00,  1.07it/s, Loss=0.5365, MSE=0.5364, PSNR=2.70dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 16:\n",
      "Tiempo: 226.87s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.478265 | Val Loss: 0.447196\n",
      "Train MSE: 0.478153 | Val MSE: 0.447110\n",
      "Train PSNR: 3.21dB | Val PSNR: 3.52dB\n",
      "Train MAE: 0.350684 | Val MAE: 0.322900\n",
      "Train Corr: 0.7695 | Val Corr: 0.8150\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.447196)\n",
      "\n",
      "Época 17/50\n",
      "--------------------------------------------------\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.447196)\n",
      "\n",
      "Época 17/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:32<00:00, 14.49it/s, Loss=0.4721, MSE=0.4720, PSNR=3.26dB]\n",
      "Training: 100%|██████████| 469/469 [00:32<00:00, 14.49it/s, Loss=0.4721, MSE=0.4720, PSNR=3.26dB]\n",
      "Validation: 100%|██████████| 79/79 [00:13<00:00,  6.03it/s, Loss=0.5320, MSE=0.5319, PSNR=2.74dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 17:\n",
      "Tiempo: 45.48s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.477578 | Val Loss: 0.445542\n",
      "Train MSE: 0.477468 | Val MSE: 0.445454\n",
      "Train PSNR: 3.21dB | Val PSNR: 3.54dB\n",
      "Train MAE: 0.349898 | Val MAE: 0.323586\n",
      "Train Corr: 0.7703 | Val Corr: 0.8138\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.445542)\n",
      "\n",
      "Época 18/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [01:27<00:00,  5.34it/s, Loss=0.4955, MSE=0.4954, PSNR=3.05dB]\n",
      "Validation:   0%|          | 0/79 [00:00<?, ?it/s]\n",
      "Validation: 100%|██████████| 79/79 [00:20<00:00,  3.88it/s, Loss=0.5330, MSE=0.5329, PSNR=2.73dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 18:\n",
      "Tiempo: 108.30s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.476068 | Val Loss: 0.445134\n",
      "Train MSE: 0.475959 | Val MSE: 0.445045\n",
      "Train PSNR: 3.23dB | Val PSNR: 3.54dB\n",
      "Train MAE: 0.348436 | Val MAE: 0.323989\n",
      "Train Corr: 0.7721 | Val Corr: 0.8133\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.445134)\n",
      "\n",
      "Época 19/50\n",
      "--------------------------------------------------\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.445134)\n",
      "\n",
      "Época 19/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [01:49<00:00,  4.30it/s, Loss=0.4696, MSE=0.4695, PSNR=3.28dB]\n",
      "Training: 100%|██████████| 469/469 [01:49<00:00,  4.30it/s, Loss=0.4696, MSE=0.4695, PSNR=3.28dB]\n",
      "Validation: 100%|██████████| 79/79 [00:13<00:00,  5.81it/s, Loss=0.5316, MSE=0.5315, PSNR=2.75dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 19:\n",
      "Tiempo: 122.78s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.475175 | Val Loss: 0.444271\n",
      "Train MSE: 0.475068 | Val MSE: 0.444186\n",
      "Train PSNR: 3.23dB | Val PSNR: 3.55dB\n",
      "Train MAE: 0.347486 | Val MAE: 0.320956\n",
      "Train Corr: 0.7731 | Val Corr: 0.8183\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.444271)\n",
      "\n",
      "Época 20/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [01:53<00:00,  4.12it/s, Loss=0.4855, MSE=0.4854, PSNR=3.14dB]\n",
      "\n",
      "Validation: 100%|██████████| 79/79 [00:22<00:00,  3.55it/s, Loss=0.5304, MSE=0.5303, PSNR=2.76dB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nResultados de la época 20:\n",
      "Tiempo: 136.11s\n",
      "Learning Rate: 1.00e-03\n",
      "Train Loss: 0.474235 | Val Loss: 0.442557\n",
      "Train MSE: 0.474129 | Val MSE: 0.442470\n",
      "Train PSNR: 3.24dB | Val PSNR: 3.57dB\n",
      "Train MAE: 0.346627 | Val MAE: 0.319708\n",
      "Train Corr: 0.7743 | Val Corr: 0.8182\n",
      "Nuevo mejor modelo guardado (Val Loss: 0.442557)\n",
      "Checkpoint guardado en época 20\n",
      "\n",
      "Época 21/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 16/469 [00:20<02:58,  2.54it/s, Loss=0.4885, MSE=0.4884, PSNR=3.11dB] "
     ]
    }
   ],
   "source": [
    "# Ejecutar Entrenamiento\n",
    "\n",
    "# Verificar que todo esté configurado correctamente\n",
    "print(\"Verificando configuración antes del entrenamiento...\")\n",
    "print(f\"Modelo: {type(model).__name__} con {count_parameters(model):,} parámetros\")\n",
    "print(f\"Dispositivo: {device}\")\n",
    "print(f\"Datos: {len(train_loader)} batches de entrenamiento, {len(test_loader)} batches de validación\")\n",
    "print(f\"Optimizador: {type(optimizer).__name__}\")\n",
    "print(f\"Función de pérdida: {type(criterion).__name__}\")\n",
    "print(f\"Gradient accumulation: {config.ACCUMULATION_STEPS} steps\")\n",
    "\n",
    "# Verificar una pasada forward antes del entrenamiento\n",
    "print(\"\\\\nVerificando forward pass...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_batch, _ = next(iter(train_loader))\n",
    "    sample_batch = sample_batch.to(device)\n",
    "    reconstruction, latent = model(sample_batch)\n",
    "    \n",
    "    print(f\"Input shape: {sample_batch.shape}\")\n",
    "    print(f\"Latent shape: {latent.shape}\")\n",
    "    print(f\"Reconstruction shape: {reconstruction.shape}\")\n",
    "    \n",
    "    # Verificar rango de valores\n",
    "    print(f\"Input range: [{sample_batch.min():.3f}, {sample_batch.max():.3f}]\")\n",
    "    print(f\"Reconstruction range: [{reconstruction.min():.3f}, {reconstruction.max():.3f}]\")\n",
    "\n",
    "# Iniciar entrenamiento\n",
    "training_logger = train_autoencoder(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=config.NUM_EPOCHS,\n",
    "    accumulation_steps=config.ACCUMULATION_STEPS,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f560b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación y Visualización de Resultados\n",
    "\n",
    "def visualize_reconstructions(model: nn.Module, dataloader: DataLoader, \n",
    "                            device: torch.device, num_samples: int = 8):\n",
    "    \"\"\"\n",
    "    Visualiza reconstrucciones del autoencoder\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        dataloader: DataLoader para obtener muestras\n",
    "        device: Dispositivo de cómputo\n",
    "        num_samples: Número de muestras a visualizar\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Obtener muestras\n",
    "    data_iter = iter(dataloader)\n",
    "    images, _ = next(data_iter)\n",
    "    images = images[:num_samples].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        reconstructions, latent = model(images)\n",
    "        \n",
    "        # Desnormalizar para visualización\n",
    "        images_vis = images.cpu()\n",
    "        reconstructions_vis = reconstructions.view(-1, 1, 28, 28).cpu()\n",
    "        \n",
    "        # Crear visualización\n",
    "        fig, axes = plt.subplots(3, num_samples, figsize=(15, 6))\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Imagen original\n",
    "            axes[0, i].imshow(images_vis[i].squeeze(), cmap='gray')\n",
    "            axes[0, i].set_title('Original')\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            # Reconstrucción\n",
    "            axes[1, i].imshow(reconstructions_vis[i].squeeze(), cmap='gray')\n",
    "            axes[1, i].set_title('Reconstrucción')\n",
    "            axes[1, i].axis('off')\n",
    "            \n",
    "            # Diferencia\n",
    "            diff = torch.abs(images_vis[i] - reconstructions_vis[i])\n",
    "            axes[2, i].imshow(diff.squeeze(), cmap='hot')\n",
    "            axes[2, i].set_title('Diferencia')\n",
    "            axes[2, i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(config.SAVE_DIR, 'reconstructions.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Calcular métricas para estas muestras\n",
    "        metrics = calculate_metrics(reconstructions, images)\n",
    "        print(f\"\\\\nMétricas de las muestras visualizadas:\")\n",
    "        print(f\"MSE: {metrics['mse']:.6f}\")\n",
    "        print(f\"MAE: {metrics['mae']:.6f}\")\n",
    "        print(f\"PSNR: {metrics['psnr']:.2f} dB\")\n",
    "        print(f\"Correlación: {metrics['correlation']:.4f}\")\n",
    "\n",
    "\n",
    "def visualize_latent_space(model: nn.Module, dataloader: DataLoader, \n",
    "                          device: torch.device, num_samples: int = 1000):\n",
    "    \"\"\"\n",
    "    Visualiza el espacio latente del autoencoder\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        dataloader: DataLoader para obtener muestras\n",
    "        device: Dispositivo de cómputo\n",
    "        num_samples: Número de muestras para la visualización\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    latent_vectors = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, label) in enumerate(dataloader):\n",
    "            if len(latent_vectors) * dataloader.batch_size >= num_samples:\n",
    "                break\n",
    "                \n",
    "            data = data.to(device)\n",
    "            latent = model.encode(data)\n",
    "            \n",
    "            latent_vectors.append(latent.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "    \n",
    "    # Concatenar todos los vectores latentes\n",
    "    latent_vectors = np.concatenate(latent_vectors, axis=0)[:num_samples]\n",
    "    labels = np.concatenate(labels, axis=0)[:num_samples]\n",
    "    \n",
    "    # Reducir dimensionalidad con PCA si es necesario\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    if latent_vectors.shape[1] > 2:\n",
    "        # Primero PCA para reducir dimensionalidad\n",
    "        pca = PCA(n_components=min(50, latent_vectors.shape[1]))\n",
    "        latent_pca = pca.fit_transform(latent_vectors)\n",
    "        \n",
    "        # Luego t-SNE para visualización 2D\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "        latent_2d = tsne.fit_transform(latent_pca)\n",
    "    else:\n",
    "        latent_2d = latent_vectors\n",
    "    \n",
    "    # Visualizar\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(f'Visualización del Espacio Latente ({latent_vectors.shape[1]}D → 2D)')\n",
    "    plt.xlabel('Componente 1')\n",
    "    plt.ylabel('Componente 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(config.SAVE_DIR, 'latent_space.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\\\nInformación del espacio latente:\")\n",
    "    print(f\"- Dimensión original: {latent_vectors.shape[1]}\")\n",
    "    print(f\"- Muestras visualizadas: {len(latent_vectors)}\")\n",
    "    print(f\"- Varianza explicada por PCA: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "\n",
    "def generate_samples(model: nn.Module, device: torch.device, num_samples: int = 8):\n",
    "    \"\"\"\n",
    "    Genera nuevas muestras interpolando en el espacio latente\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        device: Dispositivo de cómputo\n",
    "        num_samples: Número de muestras a generar\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generar vectores latentes aleatorios\n",
    "        latent_samples = torch.randn(num_samples, config.LATENT_DIM).to(device)\n",
    "        \n",
    "        # Decodificar\n",
    "        generated = model.decode(latent_samples)\n",
    "        generated = generated.view(-1, 1, 28, 28).cpu()\n",
    "        \n",
    "        # Visualizar\n",
    "        fig, axes = plt.subplots(1, num_samples, figsize=(12, 2))\n",
    "        for i in range(num_samples):\n",
    "            axes[i].imshow(generated[i].squeeze(), cmap='gray')\n",
    "            axes[i].set_title(f'Generada {i+1}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.suptitle('Muestras Generadas desde el Espacio Latente')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(config.SAVE_DIR, 'generated_samples.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def interpolate_between_samples(model: nn.Module, dataloader: DataLoader, \n",
    "                              device: torch.device, num_steps: int = 8):\n",
    "    \"\"\"\n",
    "    Interpola entre dos muestras en el espacio latente\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        dataloader: DataLoader para obtener muestras\n",
    "        device: Dispositivo de cómputo\n",
    "        num_steps: Número de pasos de interpolación\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Obtener dos muestras\n",
    "    data_iter = iter(dataloader)\n",
    "    images, _ = next(data_iter)\n",
    "    sample1, sample2 = images[0:1].to(device), images[1:2].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Codificar ambas muestras\n",
    "        latent1 = model.encode(sample1)\n",
    "        latent2 = model.encode(sample2)\n",
    "        \n",
    "        # Interpolar en el espacio latente\n",
    "        alphas = torch.linspace(0, 1, num_steps).to(device)\n",
    "        interpolated_images = []\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            interpolated_latent = (1 - alpha) * latent1 + alpha * latent2\n",
    "            interpolated_image = model.decode(interpolated_latent)\n",
    "            interpolated_images.append(interpolated_image.cpu())\n",
    "        \n",
    "        # Visualizar\n",
    "        fig, axes = plt.subplots(1, num_steps, figsize=(15, 2))\n",
    "        for i, img in enumerate(interpolated_images):\n",
    "            axes[i].imshow(img.view(28, 28), cmap='gray')\n",
    "            axes[i].set_title(f'α = {alphas[i]:.2f}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.suptitle('Interpolación en el Espacio Latente')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(config.SAVE_DIR, 'interpolation.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "print(\"Funciones de evaluación implementadas:\")\n",
    "print(\"- visualize_reconstructions: Compara originales vs reconstrucciones\")\n",
    "print(\"- visualize_latent_space: Visualización 2D del espacio latente\")\n",
    "print(\"- generate_samples: Generación desde vectores latentes aleatorios\")\n",
    "print(\"- interpolate_between_samples: Interpolación en espacio latente\")\n",
    "print(\"\\\\nEjecute estas funciones después del entrenamiento para evaluar el modelo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f88f6d8",
   "metadata": {},
   "source": [
    "## Análisis Técnico: Gradient Accumulation\n",
    "\n",
    "### ¿Qué es Gradient Accumulation?\n",
    "\n",
    "**Gradient Accumulation** es una técnica de optimización que permite simular un batch size más grande sin incrementar proporcionalmente el uso de memoria. En lugar de actualizar los parámetros del modelo después de cada batch, se acumulan los gradientes durante varios batches antes de realizar la actualización.\n",
    "\n",
    "### Implementación Técnica\n",
    "\n",
    "En nuestro código, la implementación sigue estos pasos:\n",
    "\n",
    "1. **Inicialización**: `optimizer.zero_grad()` al inicio\n",
    "2. **Acumulación**: Para cada batch:\n",
    "   - Forward pass\n",
    "   - Cálculo de pérdida escalada: `loss = loss / accumulation_steps`\n",
    "   - Backward pass: `loss.backward()`\n",
    "3. **Actualización**: Cada `accumulation_steps` batches:\n",
    "   - Gradient clipping: `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)`\n",
    "   - Actualización de parámetros: `optimizer.step()`\n",
    "   - Reset de gradientes: `optimizer.zero_grad()`\n",
    "\n",
    "### Ventajas del Gradient Accumulation\n",
    "\n",
    "1. **Eficiencia de Memoria**: Permite entrenar con batch sizes efectivos grandes en hardware limitado\n",
    "2. **Estabilidad**: Gradientes más estables al promediar sobre más muestras\n",
    "3. **Mejor Convergencia**: Especialmente útil en autoencoders donde la señal puede ser débil\n",
    "4. **Flexibilidad**: Permite ajustar el batch size efectivo sin cambiar la arquitectura\n",
    "\n",
    "### Configuración Utilizada\n",
    "\n",
    "- **Batch Size Real**: 128 muestras\n",
    "- **Accumulation Steps**: 4\n",
    "- **Batch Size Efectivo**: 128 × 4 = 512 muestras\n",
    "- **Escalado de Pérdida**: `loss / 4` para mantener la misma magnitud de gradientes\n",
    "\n",
    "### Impacto en el Autoencoder\n",
    "\n",
    "Para autoencoders, el Gradient Accumulation es especialmente beneficioso porque:\n",
    "\n",
    "1. **Reconstrucción Más Estable**: Gradientes más suaves mejoran la calidad de reconstrucción\n",
    "2. **Espacio Latente Mejor Estructurado**: Menos ruido en las actualizaciones del encoder\n",
    "3. **Convergencia Más Rápida**: Menos oscilaciones durante el entrenamiento\n",
    "4. **Mejor Generalización**: El modelo aprende representaciones más robustas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b092fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Evaluación Post-Entrenamiento\n",
    "# Ejecute esta celda después de completar el entrenamiento\n",
    "\n",
    "# Cargar el mejor modelo si existe\n",
    "best_model_path = os.path.join(config.SAVE_DIR, 'best_model.pth')\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(\"Cargando el mejor modelo para evaluación...\")\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"✓ Modelo cargado desde época {checkpoint['epoch']} con pérdida {checkpoint['loss']:.6f}\")\n",
    "    \n",
    "    # Ejecutar evaluaciones\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"EVALUACIÓN DEL MODELO ENTRENADO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Visualizar reconstrucciones\n",
    "    print(\"\\\\n1. Visualizando reconstrucciones...\")\n",
    "    visualize_reconstructions(model, test_loader, device, num_samples=8)\n",
    "    \n",
    "    # 2. Visualizar espacio latente\n",
    "    print(\"\\\\n2. Visualizando espacio latente...\")\n",
    "    visualize_latent_space(model, test_loader, device, num_samples=1000)\n",
    "    \n",
    "    # 3. Generar muestras\n",
    "    print(\"\\\\n3. Generando muestras desde espacio latente...\")\n",
    "    generate_samples(model, device, num_samples=8)\n",
    "    \n",
    "    # 4. Interpolar entre muestras\n",
    "    print(\"\\\\n4. Interpolando entre muestras...\")\n",
    "    interpolate_between_samples(model, test_loader, device, num_steps=8)\n",
    "    \n",
    "    # 5. Evaluación cuantitativa final\n",
    "    print(\"\\\\n5. Evaluación cuantitativa final...\")\n",
    "    model.eval()\n",
    "    total_metrics = {'mse': 0.0, 'mae': 0.0, 'psnr': 0.0, 'correlation': 0.0}\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _ in tqdm(test_loader, desc=\"Evaluando\"):\n",
    "            data = data.to(device)\n",
    "            reconstruction, _ = model(data)\n",
    "            \n",
    "            metrics = calculate_metrics(reconstruction, data)\n",
    "            for key, value in metrics.items():\n",
    "                total_metrics[key] += value\n",
    "            num_batches += 1\n",
    "    \n",
    "    # Promediar métricas\n",
    "    final_metrics = {key: value / num_batches for key, value in total_metrics.items()}\n",
    "    \n",
    "    print(\"\\\\nMétricas finales en conjunto de validación:\")\n",
    "    print(f\"- MSE: {final_metrics['mse']:.6f}\")\n",
    "    print(f\"- MAE: {final_metrics['mae']:.6f}\")\n",
    "    print(f\"- PSNR: {final_metrics['psnr']:.2f} dB\")\n",
    "    print(f\"- Correlación: {final_metrics['correlation']:.4f}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"EVALUACIÓN COMPLETADA\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Todos los resultados guardados en: {config.SAVE_DIR}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"No se encontró modelo entrenado en {best_model_path}\")\n",
    "    print(\"Ejecute primero el entrenamiento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2115f95a",
   "metadata": {},
   "source": [
    "## Conclusiones y Próximos Pasos\n",
    "\n",
    "### Resumen de la Implementación\n",
    "\n",
    "Se ha implementado exitosamente un **Autoencoder robusto para MNIST** con las siguientes características técnicas:\n",
    "\n",
    "#### Arquitectura del Modelo\n",
    "- **Encoder**: 784 → 512 → 256 → 128 → 64 → 32 (espacio latente)\n",
    "- **Decoder**: 32 → 64 → 128 → 256 → 512 → 784 (reconstrucción)\n",
    "- **Activaciones**: ReLU para capas ocultas, Tanh para salida\n",
    "- **Regularización**: BatchNorm y Dropout (0.2) para estabilidad\n",
    "\n",
    "#### Gradient Accumulation\n",
    "- **Implementación Profesional**: Accumulation steps = 4\n",
    "- **Batch Size Efectivo**: 512 (128 × 4)\n",
    "- **Escalado de Pérdida**: Correctamente implementado\n",
    "- **Gradient Clipping**: max_norm = 1.0 para estabilidad\n",
    "\n",
    "#### Training Loop Avanzado\n",
    "- **Métricas Completas**: MSE, MAE, PSNR, Correlación\n",
    "- **Logging Detallado**: Progreso en tiempo real\n",
    "- **Checkpointing**: Guardado automático del mejor modelo\n",
    "- **Learning Rate Scheduling**: ReduceLROnPlateau\n",
    "- **Early Stopping**: Basado en learning rate mínimo\n",
    "\n",
    "### Ventajas de esta Implementación\n",
    "\n",
    "1. **Eficiencia de Memoria**: Gradient accumulation permite entrenar con batch sizes grandes\n",
    "2. **Estabilidad de Entrenamiento**: BatchNorm, Dropout y gradient clipping\n",
    "3. **Monitoreo Completo**: Múltiples métricas y visualizaciones\n",
    "4. **Reproducibilidad**: Seeds fijos y configuración centralizada\n",
    "5. **Escalabilidad**: Código modular y bien estructurado\n",
    "\n",
    "### Próximos Pasos Sugeridos\n",
    "\n",
    "#### Mejoras del Modelo\n",
    "1. **Autoencoder Variacional (VAE)**: Agregar distribuciones probabilísticas\n",
    "2. **Convolutional Autoencoder**: Explotar estructura espacial de imágenes\n",
    "3. **Adversarial Training**: Implementar GAN-based autoencoders\n",
    "4. **Attention Mechanisms**: Mejorar la capacidad de reconstrucción\n",
    "\n",
    "#### Optimizaciones Técnicas\n",
    "1. **Mixed Precision Training**: Usar FP16 para mayor eficiencia\n",
    "2. **Data Augmentation**: Mejorar generalización\n",
    "3. **Ensemble Methods**: Combinar múltiples autoencoders\n",
    "4. **Hyperparameter Tuning**: Optimización automática con Optuna\n",
    "\n",
    "#### Aplicaciones Extendidas\n",
    "1. **Detección de Anomalías**: Usar error de reconstrucción\n",
    "2. **Denoising**: Entrenar con imágenes ruidosas\n",
    "3. **Transfer Learning**: Adaptar a otros datasets\n",
    "4. **Compression**: Optimizar para aplicaciones de compresión\n",
    "\n",
    "### Consideraciones de Producción\n",
    "\n",
    "Para llevar este modelo a producción:\n",
    "\n",
    "1. **Optimización**: Quantización y pruning del modelo\n",
    "2. **Serving**: Implementación con FastAPI/Flask\n",
    "3. **Monitoring**: MLflow para tracking de experimentos\n",
    "4. **Testing**: Unit tests y integration tests\n",
    "5. **Deployment**: Docker containers y Kubernetes\n",
    "\n",
    "Esta implementación proporciona una base sólida para autoencoders en aplicaciones reales, con énfasis en mejores prácticas de ingeniería de machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
